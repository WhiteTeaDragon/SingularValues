{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SingularClippingConvTT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Zjepj31fVNww",
        "ZIOsm1xkDbtG",
        "KSatFAHUVVZM",
        "wfvhZ-l0rsUd",
        "NRChluDlrVYz"
      ],
      "authorship_tag": "ABX9TyOFXHjUFQyyt3eoSTIp47Jn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WhiteTeaDragon/SingularValues/blob/main/SingularClippingConvTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "praf7vwRHu5P"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import regularizers\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
        "import six\n",
        "import functools\n",
        "from tensorflow.python.ops import nn, nn_ops\n",
        "import keras.backend as K\n",
        "from keras.callbacks import LambdaCallback\n",
        "import copy\n",
        "import keras\n",
        "\n",
        "np.random.seed(1)   \n",
        "rn.seed(1)   \n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjepj31fVNww"
      },
      "source": [
        "### Функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ0P4VJ6Li7H"
      },
      "source": [
        "def full_tt(K1, K2, K3):\n",
        "    \"\"\"Converts a TensorTrain into a regular tensor or matrix (tf.Tensor).\"\"\"\n",
        "    res = K1\n",
        "    K2_reshaped = tf.reshape(K2, (K2.shape[0], -1))\n",
        "    res = tf.matmul(res, K2_reshaped)\n",
        "    res = tf.reshape(res, (-1, K3.shape[0]))\n",
        "    res = tf.matmul(res, K3)\n",
        "    res = tf.reshape(res, (K1.shape[0],) + K2.shape[1:-1] + (K3.shape[-1],))\n",
        "    num_dims = len(K2.shape[1:-1])\n",
        "    return tf.transpose(res, list(range(1, num_dims + 1)) + [0, num_dims + 1])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMHS184LtygX"
      },
      "source": [
        "def Clip_OperatorNorm(conv, inp_shape, clip_to):\n",
        "    conv_tr = tf.cast(tf.transpose(conv, perm=[2, 3, 0, 1]), tf.complex128)\n",
        "    conv_shape = conv.get_shape().as_list()\n",
        "    padding = tf.constant([[0, 0], [0, 0],\n",
        "                            [0, inp_shape[0] - conv_shape[0]],\n",
        "                            [0, inp_shape[1] - conv_shape[1]]])\n",
        "    transform_coeff = tf.signal.fft2d(tf.pad(conv_tr, padding))\n",
        "    D, U, V = tf.linalg.svd(tf.transpose(transform_coeff, perm = [2, 3, 0, 1]))\n",
        "    norm = tf.reduce_max(D)\n",
        "    D_clipped = tf.cast(tf.minimum(D, clip_to), tf.complex128)\n",
        "    clipped_coeff = tf.matmul(U, tf.matmul(tf.linalg.diag(D_clipped),\n",
        "                                            V, adjoint_b=True))\n",
        "    clipped_conv_padded = tf.math.real(tf.signal.ifft2d(\n",
        "        tf.transpose(clipped_coeff, perm=[2, 3, 0, 1])))\n",
        "    return tf.slice(tf.transpose(clipped_conv_padded, perm=[2, 3, 0, 1]),\n",
        "                    [0] * len(conv_shape), conv_shape), norm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKboVaWmcytr"
      },
      "source": [
        "class Clipping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, clip_to):\n",
        "        tf.keras.callbacks.Callback.__init__(self)\n",
        "        self.clip_to = clip_to\n",
        "\n",
        "    def on_epoch_end(self, epochs, logs={}):\n",
        "        for layer in self.model.layers:\n",
        "            if layer.name[:17] == \"conv_decomposed2d\":\n",
        "                # (m, R) -> (m, min(m, R)) -- q: q.T @ q = I, (min(m, R), R) -- r\n",
        "                q1, r1 = tf.linalg.qr(layer.K1, full_matrices=False)\n",
        "                if layer.K1.shape[0] < layer.K1.shape[1]:\n",
        "                    padding_q = tf.constant(\n",
        "                        [[0, 0], [0, layer.K1.shape[1] - layer.K1.shape[0]]])\n",
        "                    q1 = tf.pad(q1, padding_q)\n",
        "                    padding_r = tf.constant(\n",
        "                        [[0, layer.K1.shape[1] - layer.K1.shape[0]], [0, 0]])\n",
        "                    r1 = tf.pad(r1, padding_r)\n",
        "                # now q1 has shape (m, R), r1 -- (R, R)\n",
        "                K.set_value(layer.K1, q1)\n",
        "                # (m, R) -> (m, min(m, R)) -- q: q.T @ q = I, (min(m, R), R) -- r\n",
        "                q2, r2 = tf.linalg.qr(tf.transpose(layer.K3), full_matrices=False)\n",
        "                if layer.K3.shape[1] < layer.K3.shape[0]:\n",
        "                    padding_q = tf.constant(\n",
        "                        [[0, 0], [0, layer.K1.shape[0] - layer.K1.shape[1]]])\n",
        "                    q2 = tf.pad(q2, padding_q)\n",
        "                    padding_r = tf.constant(\n",
        "                        [[0, layer.K1.shape[0] - layer.K1.shape[1]], [0, 0]])\n",
        "                    r2 = tf.pad(r2, padding_r)\n",
        "                # now q2 has shape (m, R), r2 -- (R, R)\n",
        "                K.set_value(layer.K3, tf.transpose(q2))\n",
        "                middle_k = full_tt(r1, layer.K2, r2)\n",
        "                K.set_value(layer.K2, tf.transpose(Clip_OperatorNorm(middle_k,\n",
        "                                                        layer.input_shape[1:3],\n",
        "                                                        self.clip_to)[0], perm=[2, 0, 1, 3]))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIOsm1xkDbtG"
      },
      "source": [
        "### Свёрточный 2D слой с ТТ-разложением"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwjwq1q3FtSB"
      },
      "source": [
        "class ConvDecomposed2D(tf.keras.layers.Conv2D):\n",
        "    def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               decomposition_rank,\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               bias_initializer='zeros',\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "        super(ConvDecomposed2D, self).__init__(\n",
        "               filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=strides,\n",
        "               padding=padding,\n",
        "               data_format=data_format,\n",
        "               dilation_rate=dilation_rate,\n",
        "               groups=1, # does not support groups!\n",
        "               activation=activation,\n",
        "               use_bias=use_bias,\n",
        "               kernel_initializer=kernel_initializer,\n",
        "               bias_initializer=bias_initializer,\n",
        "               kernel_regularizer=kernel_regularizer,\n",
        "               bias_regularizer=bias_regularizer,\n",
        "               activity_regularizer=activity_regularizer,\n",
        "               kernel_constraint=kernel_constraint,\n",
        "               bias_constraint=bias_constraint,\n",
        "               **kwargs)\n",
        "        self.decomposition_rank = decomposition_rank\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        input_channel = self._get_input_channel(input_shape)\n",
        "        kernel_shape = self.kernel_size + (input_channel, self.filters)\n",
        "    \n",
        "        self.K1 = self.add_weight(\n",
        "            name='K1',\n",
        "            shape=(input_channel, self.decomposition_rank),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "        self.K2 = self.add_weight(\n",
        "            name='K2',\n",
        "            shape=(self.decomposition_rank,) + self.kernel_size + (\n",
        "                                                    self.decomposition_rank,),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "        self.K3 = self.add_weight(\n",
        "            name='K3',\n",
        "            shape=(self.decomposition_rank, self.filters),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "        \n",
        "        # the rest is copied from Conv build function\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "              name='bias',\n",
        "              shape=(self.filters,),\n",
        "              initializer=self.bias_initializer,\n",
        "              regularizer=self.bias_regularizer,\n",
        "              constraint=self.bias_constraint,\n",
        "              trainable=True,\n",
        "              dtype=self.dtype)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        channel_axis = self._get_channel_axis()\n",
        "        self.input_spec = InputSpec(min_ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_channel})\n",
        "    \n",
        "        # Convert Keras formats to TF native formats.\n",
        "        if self.padding == 'causal':\n",
        "            tf_padding = 'VALID'  # Causal padding handled in `call`.\n",
        "        elif isinstance(self.padding, six.string_types):\n",
        "            tf_padding = self.padding.upper()\n",
        "        else:\n",
        "            tf_padding = self.padding\n",
        "        tf_dilations = list(self.dilation_rate)\n",
        "        tf_strides = list(self.strides)\n",
        "    \n",
        "        tf_op_name = self.__class__.__name__\n",
        "        if tf_op_name == 'Conv1D':\n",
        "            tf_op_name = 'conv1d'  # Backwards compat.\n",
        "    \n",
        "        self._convolution_op = functools.partial(\n",
        "            nn_ops.convolution_v2,\n",
        "            strides=tf_strides,\n",
        "            padding=tf_padding,\n",
        "            dilations=tf_dilations,\n",
        "            data_format=self._tf_data_format,\n",
        "            name=tf_op_name)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self._convolution_op(inputs, full_tt(self.K1, self.K2, self.K3))\n",
        "        if self.use_bias:\n",
        "            output_rank = outputs.shape.rank\n",
        "            if self.rank == 1 and self._channels_first:\n",
        "                # nn.bias_add does not accept a 1D input tensor.\n",
        "                bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
        "                outputs += bias\n",
        "            else:\n",
        "                # Handle multiple batch dimensions.\n",
        "                if output_rank is not None and output_rank > 2 + self.rank:\n",
        "    \n",
        "                    def _apply_fn(o):\n",
        "                        return nn.bias_add(o, self.bias, data_format=self._tf_data_format)\n",
        "    \n",
        "                    outputs = nn_ops.squeeze_batch_dims(\n",
        "                      outputs, _apply_fn, inner_rank=self.rank + 1)\n",
        "                else:\n",
        "                    outputs = nn.bias_add(\n",
        "                      outputs, self.bias, data_format=self._tf_data_format)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSatFAHUVVZM"
      },
      "source": [
        "### Проверка работоспособности слоя на наборе данных из букв (одна из наших домашек)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfvhZ-l0rsUd"
      },
      "source": [
        "#### Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3tYiEG_VTGT",
        "outputId": "1c9baf68-97de-4bc4-8328-bb1360a82cc9"
      },
      "source": [
        "!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz\n",
        "!tar -xvf notMNIST_large.tar.gz >> /dev/null"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-02 12:55:17--  http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz\n",
            "Resolving yaroslavvb.com (yaroslavvb.com)... 129.121.4.193\n",
            "Connecting to yaroslavvb.com (yaroslavvb.com)|129.121.4.193|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247336696 (236M) [application/x-gzip]\n",
            "Saving to: ‘notMNIST_large.tar.gz’\n",
            "\n",
            "notMNIST_large.tar. 100%[===================>] 235.88M  56.1MB/s    in 4.5s    \n",
            "\n",
            "2021-05-02 12:55:22 (52.1 MB/s) - ‘notMNIST_large.tar.gz’ saved [247336696/247336696]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxgpV6QQVeQU"
      },
      "source": [
        "DATA_DIR = 'notMNIST_large/'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo8NwEd7Vrcu"
      },
      "source": [
        "from glob import glob\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "for img_path in glob(f'{DATA_DIR}/**/*.png'):\n",
        "  try:\n",
        "    img = Image.open(img_path)\n",
        "  except:\n",
        "      os.remove(img_path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "PAIaOAt4Vvji",
        "outputId": "42551802-1911-4973-e478-ab70b5f9b54f"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "plt.figure(figsize=(17,10))\n",
        "letter = 'A'\n",
        "img = cv2.imread(os.path.join(DATA_DIR, letter, os.listdir(f'{DATA_DIR}/{letter}/')[0]))\n",
        "plt.imshow(img)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6d980e1250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1224x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v28-MI9WWPLn"
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.layers.core import Activation, Reshape, Dense, Flatten\n",
        "from keras.layers import Conv2D, MaxPool2D, InputLayer, BatchNormalization, Dropout, GlobalAveragePooling2D, GlobalMaxPool2D\n",
        "from keras.models import Model\n",
        "\n",
        "pic_size = 28\n",
        "n_classes = len(os.listdir(DATA_DIR))\n",
        "\n",
        "def build_model(decomposition_rank):\n",
        "    model = keras.Sequential([\n",
        "        ConvDecomposed2D(32, 3, decomposition_rank=decomposition_rank,\n",
        "                        input_shape=(pic_size, pic_size, 3),\n",
        "                            data_format=\"channels_last\", activation='relu',\n",
        "                            padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        ConvDecomposed2D(32, 3, decomposition_rank=decomposition_rank,\n",
        "                        activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        ConvDecomposed2D(32, 3, decomposition_rank=decomposition_rank,\n",
        "                        activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(128, activation='relu'),\n",
        "        keras.layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3EyzG5DWuER",
        "outputId": "2127f324-0d7d-49a8-d8b5-70b3e9c81534"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Подсказка: train/val split удобно делать вот так https://stackoverflow.com/questions/53037510/can-flow-from-directory-get-train-and-validation-data-from-the-same-directory-in (самый залайканый ответ)\n",
        "\n",
        "\"\"\" Data generators initialization: for train and validation sets \"\"\"\n",
        "generator = ImageDataGenerator(validation_split=0.1, rescale=1./255)\n",
        "train_gen = generator.flow_from_directory(DATA_DIR,\n",
        "                                          target_size=(pic_size, pic_size),\n",
        "                                          class_mode='categorical',\n",
        "                                          subset='training', seed=1)\n",
        "val_gen = generator.flow_from_directory(DATA_DIR,\n",
        "                                        target_size=(pic_size, pic_size),\n",
        "                                        class_mode='categorical',\n",
        "                                        subset='validation', seed=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 476205 images belonging to 10 classes.\n",
            "Found 52909 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRChluDlrVYz"
      },
      "source": [
        "#### decomposition_rank = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUuo_lwPigPL",
        "outputId": "84fea738-aedd-4416-e910-8e4ff85c8037"
      },
      "source": [
        "model1 = build_model(1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_decomposed2d_12 (ConvDe (None, 28, 28, 32)        76        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv_decomposed2d_13 (ConvDe (None, 14, 14, 32)        105       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv_decomposed2d_14 (ConvDe (None, 7, 7, 32)          105       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               36992     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 38,568\n",
            "Trainable params: 38,568\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR4MyUYOburL",
        "outputId": "6df17aba-05f3-4f2e-e1cd-25414ace3625"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history2 = model1.fit(train_gen, steps_per_epoch=step_size_train, epochs=5,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid, callbacks=[Clipping(0.5)])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "14881/14881 [==============================] - 208s 14ms/step - loss: 0.8122 - accuracy: 0.7511 - val_loss: 0.4802 - val_accuracy: 0.8559\n",
            "Epoch 2/5\n",
            "14881/14881 [==============================] - 208s 14ms/step - loss: 0.8415 - accuracy: 0.7172 - val_loss: 0.4669 - val_accuracy: 0.8595\n",
            "Epoch 3/5\n",
            "14881/14881 [==============================] - 208s 14ms/step - loss: 0.7473 - accuracy: 0.7533 - val_loss: 0.4359 - val_accuracy: 0.8678\n",
            "Epoch 4/5\n",
            "14881/14881 [==============================] - 208s 14ms/step - loss: 0.6514 - accuracy: 0.7931 - val_loss: 0.4284 - val_accuracy: 0.8686\n",
            "Epoch 5/5\n",
            "14881/14881 [==============================] - 207s 14ms/step - loss: 0.6331 - accuracy: 0.7999 - val_loss: 0.4323 - val_accuracy: 0.8676\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}