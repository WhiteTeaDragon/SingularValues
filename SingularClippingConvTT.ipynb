{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SingularClippingConvTT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMw7zDNe1I4a1L57rsm/60q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WhiteTeaDragon/SingularValues/blob/main/SingularClippingConvTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "praf7vwRHu5P"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import regularizers\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
        "import six\n",
        "import functools\n",
        "from tensorflow.python.ops import nn, nn_ops\n",
        "import keras.backend as K\n",
        "from keras.callbacks import LambdaCallback\n",
        "import copy\n",
        "import keras\n",
        "\n",
        "np.random.seed(1)   \n",
        "rn.seed(1)   \n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjepj31fVNww"
      },
      "source": [
        "### Функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ0P4VJ6Li7H"
      },
      "source": [
        "def full_tt(K1, K2, K3):\n",
        "    \"\"\"Converts a TensorTrain into a regular tensor or matrix (tf.Tensor).\"\"\"\n",
        "    res = K1\n",
        "    K2_reshaped = tf.reshape(K2, (K2.shape[0], -1))\n",
        "    res = tf.matmul(res, K2_reshaped)\n",
        "    res = tf.reshape(res, (-1, K3.shape[0]))\n",
        "    res = tf.matmul(res, K3)\n",
        "    res = tf.reshape(res, (K1.shape[0],) + K2.shape[1:-1] + (K3.shape[-1],))\n",
        "    num_dims = len(K2.shape[1:-1])\n",
        "    return tf.transpose(res, list(range(1, num_dims + 1)) + [0, num_dims + 1])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMHS184LtygX"
      },
      "source": [
        "def Clip_OperatorNorm(conv, inp_shape, clip_to):\n",
        "    conv_tr = tf.cast(tf.transpose(conv, perm=[2, 3, 0, 1]), tf.complex128)\n",
        "    conv_shape = conv.get_shape().as_list()\n",
        "    padding = tf.constant([[0, 0], [0, 0],\n",
        "                            [0, inp_shape[0] - conv_shape[0]],\n",
        "                            [0, inp_shape[1] - conv_shape[1]]])\n",
        "    transform_coeff = tf.signal.fft2d(tf.pad(conv_tr, padding))\n",
        "    D, U, V = tf.linalg.svd(tf.transpose(transform_coeff, perm = [2, 3, 0, 1]))\n",
        "    norm = tf.reduce_max(D)\n",
        "    D_clipped = tf.cast(tf.minimum(D, clip_to), tf.complex128)\n",
        "    clipped_coeff = tf.matmul(U, tf.matmul(tf.linalg.diag(D_clipped),\n",
        "                                            V, adjoint_b=True))\n",
        "    clipped_conv_padded = tf.math.real(tf.signal.ifft2d(\n",
        "        tf.transpose(clipped_coeff, perm=[2, 3, 0, 1])))\n",
        "    return tf.slice(tf.transpose(clipped_conv_padded, perm=[2, 3, 0, 1]),\n",
        "                    [0] * len(conv_shape), conv_shape), norm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKboVaWmcytr"
      },
      "source": [
        "class Clipping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, clip_to):\n",
        "        tf.keras.callbacks.Callback.__init__(self)\n",
        "        self.clip_to = clip_to\n",
        "\n",
        "    @staticmethod\n",
        "    def get_new_K(K1, K2, K3):\n",
        "        # (m, R) -> (m, min(m, R)) -- q: q.T @ q = I, (min(m, R), R) -- r\n",
        "        q1, r1 = tf.linalg.qr(K1, full_matrices=False)\n",
        "        q3, r3 = tf.linalg.qr(tf.transpose(K3), full_matrices=False)\n",
        "        middle_k = full_tt(r1, K2, tf.transpose(r3))\n",
        "        return q1, middle_k, tf.transpose(q3)\n",
        "\n",
        "    def on_epoch_end(self, epochs, logs=None):\n",
        "        for layer in self.model.layers:\n",
        "            if layer.name[:17] == \"conv_decomposed2d\":\n",
        "                K1, K2, K3 = self.get_new_K(layer.K1, layer.K2, layer.K3)\n",
        "                K2 = tf.transpose(Clip_OperatorNorm(K2, layer.input_shape[1:3],\n",
        "                                                        self.clip_to)[0],\n",
        "                                  perm=[2, 0, 1, 3])\n",
        "                K.set_value(layer.K1, K1)\n",
        "                K.set_value(layer.K3, K3)\n",
        "                K.set_value(layer.K2, K2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpzZjAhrrPxN"
      },
      "source": [
        "### Проверка Clipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Zqx3yWrTHR"
      },
      "source": [
        "def singular_values(conv):\n",
        "    inp_shape = (100, 100)\n",
        "    conv_tr = tf.cast(tf.transpose(conv, perm=[2, 3, 0, 1]), tf.complex128)\n",
        "    conv_shape = conv.get_shape().as_list()\n",
        "    padding = tf.constant([[0, 0], [0, 0],\n",
        "                            [0, inp_shape[0] - conv_shape[0]],\n",
        "                            [0, inp_shape[1] - conv_shape[1]]])\n",
        "    transform_coeff = tf.signal.fft2d(tf.pad(conv_tr, padding))\n",
        "    s = tf.linalg.svd(tf.transpose(transform_coeff, perm = [2, 3, 0, 1]),\n",
        "                            compute_uv=False)\n",
        "    return s\n",
        "\n",
        "def clever_singular_values(K1, K2, K3):\n",
        "    K1, K2, K3 = Clipping.get_new_K(K1, K2, K3)\n",
        "    return singular_values(K2)\n",
        "\n",
        "def simple_singular_values(K1, K2, K3):\n",
        "    return singular_values(full_tt(K1, K2, K3))\n",
        "\n",
        "def check_funcs(rank, kernel_shape, filter_shape):\n",
        "    r1 = min(filter_shape[0], rank)\n",
        "    r2 = min(filter_shape[1], rank)\n",
        "    K1 = np.random.rand(*(filter_shape[0], r1))\n",
        "    K2 = np.random.rand(*((r1,) + kernel_shape + (r2,)))\n",
        "    K3 = np.random.rand(*(r2, filter_shape[1]))\n",
        "    clever_wrong = clever_singular_values(K1, K2, K3)\n",
        "    clever_wrong = np.array(sorted(list(np.array(clever_wrong).ravel())))\n",
        "    clever_wrong = clever_wrong[np.abs(clever_wrong) > 1e-8]\n",
        "    simple_right = simple_singular_values(K1, K2, K3)\n",
        "    simple_right = np.array(sorted(list(np.array(simple_right).ravel())))\n",
        "    simple_right = simple_right[np.abs(simple_right) > 1e-8]\n",
        "    # print(len(clever_wrong), len(simple_right))\n",
        "    return np.allclose(clever_wrong, simple_right)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmFznn2PxCMd",
        "outputId": "81aad56e-4a65-4275-9a38-378c149b2207"
      },
      "source": [
        "check_funcs(2, (2, 2), (2, 2))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfrIv6rj3bPp",
        "outputId": "586a416f-4024-4998-ecee-621ef972862d"
      },
      "source": [
        "check_funcs(4, (2, 2), (3, 5))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxAxnLlu79c9",
        "outputId": "244a4bc0-1961-40ff-cddf-affa472c5e6e"
      },
      "source": [
        "check_funcs(4, (2, 1), (3, 5))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7V2Eusx7_jt",
        "outputId": "450ba1c5-d38d-4d8c-a6bf-53fa7f2f5eb0"
      },
      "source": [
        "check_funcs(4, (2, 2), (5, 3))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwf-NjvL8Ba_",
        "outputId": "7ce990f9-d168-4d0e-c565-e0cd4cb9369f"
      },
      "source": [
        "check_funcs(4, (2, 1), (5, 3))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtZDOnQI8EjV",
        "outputId": "38650023-b681-42fd-c455-bd6eb5a9e506"
      },
      "source": [
        "check_funcs(4, (1, 2), (3, 5))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGvvQINF8Gf2",
        "outputId": "b1e00d7a-c8bc-4319-896b-db7350263ec1"
      },
      "source": [
        "check_funcs(2, (2, 2), (3, 5))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia-CFP0T2b2B",
        "outputId": "c6223d16-253d-4636-e92b-2012ed1dacdc"
      },
      "source": [
        "check_funcs(4, (7, 7), (32, 32))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIOsm1xkDbtG"
      },
      "source": [
        "### Свёрточный 2D слой с ТТ-разложением"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwjwq1q3FtSB"
      },
      "source": [
        "class ConvDecomposed2D(tf.keras.layers.Conv2D):\n",
        "    def __init__(self,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 decomposition_rank,\n",
        "                 strides=(1, 1),\n",
        "                 padding='valid',\n",
        "                 data_format=None,\n",
        "                 dilation_rate=(1, 1),\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(ConvDecomposed2D, self).__init__(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding=padding,\n",
        "            data_format=data_format,\n",
        "            dilation_rate=dilation_rate,\n",
        "            groups=1,  # does not support groups!\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            **kwargs)\n",
        "        self.decomposition_rank = decomposition_rank\n",
        "        self.K1 = None\n",
        "        self.K2 = None\n",
        "        self.K3 = None\n",
        "        self.bias = None\n",
        "        self._convolution_op = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        input_channel = self._get_input_channel(input_shape)\n",
        "        r1 = min(input_channel, self.decomposition_rank)\n",
        "        r2 = min(self.filters, self.decomposition_rank)\n",
        "\n",
        "        self.K1 = self.add_weight(\n",
        "            name='K1',\n",
        "            shape=(input_channel, r1),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "        self.K2 = self.add_weight(\n",
        "            name='K2',\n",
        "            shape=(r1,) + self.kernel_size + (r2,),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "        self.K3 = self.add_weight(\n",
        "            name='K3',\n",
        "            shape=(r2, self.filters),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "\n",
        "        # the rest is copied from Conv build function\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                name='bias',\n",
        "                shape=(self.filters,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                trainable=True,\n",
        "                dtype=self.dtype)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        channel_axis = self._get_channel_axis()\n",
        "        self.input_spec = InputSpec(min_ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_channel})\n",
        "\n",
        "        # Convert Keras formats to TF native formats.\n",
        "        if self.padding == 'causal':\n",
        "            tf_padding = 'VALID'  # Causal padding handled in `call`.\n",
        "        elif isinstance(self.padding, six.string_types):\n",
        "            tf_padding = self.padding.upper()\n",
        "        else:\n",
        "            tf_padding = self.padding\n",
        "        tf_dilations = list(self.dilation_rate)\n",
        "        tf_strides = list(self.strides)\n",
        "\n",
        "        tf_op_name = self.__class__.__name__\n",
        "        if tf_op_name == 'Conv1D':\n",
        "            tf_op_name = 'conv1d'  # Backwards compat.\n",
        "\n",
        "        self._convolution_op = functools.partial(\n",
        "            nn_ops.convolution_v2,\n",
        "            strides=tf_strides,\n",
        "            padding=tf_padding,\n",
        "            dilations=tf_dilations,\n",
        "            data_format=self._tf_data_format,\n",
        "            name=tf_op_name)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self._convolution_op(inputs,\n",
        "                                       full_tt(self.K1, self.K2, self.K3))\n",
        "        if self.use_bias:\n",
        "            output_rank = outputs.shape.rank\n",
        "            if self.rank == 1 and self._channels_first:\n",
        "                # nn.bias_add does not accept a 1D input tensor.\n",
        "                bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
        "                outputs += bias\n",
        "            else:\n",
        "                # Handle multiple batch dimensions.\n",
        "                if output_rank is not None and output_rank > 2 + self.rank:\n",
        "\n",
        "                    def _apply_fn(o):\n",
        "                        return nn.bias_add(o, self.bias,\n",
        "                                           data_format=self._tf_data_format)\n",
        "\n",
        "                    outputs = nn_ops.squeeze_batch_dims(\n",
        "                        outputs, _apply_fn, inner_rank=self.rank + 1)\n",
        "                else:\n",
        "                    outputs = nn.bias_add(\n",
        "                        outputs, self.bias, data_format=self._tf_data_format)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSatFAHUVVZM"
      },
      "source": [
        "### Проверка работоспособности слоя на наборе данных из букв (одна из наших домашек)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfvhZ-l0rsUd"
      },
      "source": [
        "#### Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3tYiEG_VTGT",
        "outputId": "335656fc-54e6-43ae-fce4-f0314ade5f34"
      },
      "source": [
        "!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz\n",
        "!tar -xvf notMNIST_large.tar.gz >> /dev/null"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-04 16:30:10--  http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz\n",
            "Resolving yaroslavvb.com (yaroslavvb.com)... 129.121.4.193\n",
            "Connecting to yaroslavvb.com (yaroslavvb.com)|129.121.4.193|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247336696 (236M) [application/x-gzip]\n",
            "Saving to: ‘notMNIST_large.tar.gz’\n",
            "\n",
            "notMNIST_large.tar. 100%[===================>] 235.88M  70.5MB/s    in 3.6s    \n",
            "\n",
            "2021-05-04 16:30:14 (65.9 MB/s) - ‘notMNIST_large.tar.gz’ saved [247336696/247336696]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxgpV6QQVeQU"
      },
      "source": [
        "DATA_DIR = 'notMNIST_large/'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo8NwEd7Vrcu"
      },
      "source": [
        "from glob import glob\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "for img_path in glob(f'{DATA_DIR}/**/*.png'):\n",
        "  try:\n",
        "    img = Image.open(img_path)\n",
        "  except:\n",
        "      os.remove(img_path)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "PAIaOAt4Vvji",
        "outputId": "f1da307b-7c67-4951-9bfa-d1893425336e"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "plt.figure(figsize=(17,10))\n",
        "letter = 'A'\n",
        "img = cv2.imread(os.path.join(DATA_DIR, letter, os.listdir(f'{DATA_DIR}/{letter}/')[0]))\n",
        "plt.imshow(img)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0b049bd50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1224x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v28-MI9WWPLn"
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.layers.core import Activation, Reshape, Dense, Flatten\n",
        "from keras.layers import Conv2D, MaxPool2D, InputLayer, BatchNormalization, Dropout, GlobalAveragePooling2D, GlobalMaxPool2D\n",
        "from keras.models import Model\n",
        "\n",
        "pic_size = 28\n",
        "n_classes = len(os.listdir(DATA_DIR))\n",
        "\n",
        "def build_model(decomposition_rank):\n",
        "    model = keras.Sequential([\n",
        "        ConvDecomposed2D(32, 3, decomposition_rank=decomposition_rank,\n",
        "                        input_shape=(pic_size, pic_size, 3),\n",
        "                            data_format=\"channels_last\", activation='relu',\n",
        "                            padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        ConvDecomposed2D(32, 3, decomposition_rank=decomposition_rank,\n",
        "                        activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        ConvDecomposed2D(16, 3, decomposition_rank=decomposition_rank,\n",
        "                        activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(128, activation='relu'),\n",
        "        keras.layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3EyzG5DWuER",
        "outputId": "4164f107-590d-4aca-a474-7e0cc4259c78"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Подсказка: train/val split удобно делать вот так https://stackoverflow.com/questions/53037510/can-flow-from-directory-get-train-and-validation-data-from-the-same-directory-in (самый залайканый ответ)\n",
        "\n",
        "\"\"\" Data generators initialization: for train and validation sets \"\"\"\n",
        "generator = ImageDataGenerator(validation_split=0.1, rescale=1./255)\n",
        "train_gen = generator.flow_from_directory(DATA_DIR,\n",
        "                                          target_size=(pic_size, pic_size),\n",
        "                                          class_mode='categorical',\n",
        "                                          subset='training', seed=1)\n",
        "val_gen = generator.flow_from_directory(DATA_DIR,\n",
        "                                        target_size=(pic_size, pic_size),\n",
        "                                        class_mode='categorical',\n",
        "                                        subset='validation', seed=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 476205 images belonging to 10 classes.\n",
            "Found 52909 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRChluDlrVYz"
      },
      "source": [
        "#### decomposition_rank = 17"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUuo_lwPigPL",
        "outputId": "5b5ca3d6-e6cb-48bd-e9c1-ae5ba4812434"
      },
      "source": [
        "model1 = build_model(17)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_decomposed2d (ConvDecom (None, 28, 28, 32)        1044      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv_decomposed2d_1 (ConvDec (None, 14, 14, 32)        3721      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv_decomposed2d_2 (ConvDec (None, 7, 7, 16)          3264      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 144)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               18560     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 27,879\n",
            "Trainable params: 27,879\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR4MyUYOburL",
        "outputId": "cd68e986-7500-4d37-bafd-e6ddc1936c41"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history2 = model1.fit(train_gen, steps_per_epoch=step_size_train, epochs=2,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid, callbacks=[Clipping(0.5)])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "14881/14881 [==============================] - 196s 11ms/step - loss: 0.4638 - accuracy: 0.8611 - val_loss: 0.2754 - val_accuracy: 0.9175\n",
            "Epoch 2/2\n",
            "14881/14881 [==============================] - 157s 11ms/step - loss: 0.4092 - accuracy: 0.8698 - val_loss: 0.2394 - val_accuracy: 0.9278\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}