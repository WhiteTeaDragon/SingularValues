{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CircConvClip.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "praf7vwRHu5P"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import regularizers\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
        "import six\n",
        "import functools\n",
        "from tensorflow.python.ops import nn, nn_ops\n",
        "\n",
        "np.random.seed(1)\n",
        "rn.seed(1)   \n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIOsm1xkDbtG"
      },
      "source": [
        "### Циркулянтный свёрточный 2D слой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taBJbgcm__kY"
      },
      "source": [
        "def full_circ(kernel, input_shape):\n",
        "    \"\"\"Converts the truncated circular version of kernel with size k k rn s into the full version with size k k rn sn.\"\"\"\n",
        "\n",
        "    m, _, r, s, n = kernel.shape\n",
        "\n",
        "    full_kernel = tf.stack([tf.roll(kernel, i, -1) for i in range(n)], axis=-1)\n",
        "    full_kernel = tf.reshape(tf.transpose(full_kernel, [0, 1, 2, 4, 3, 5]), (m, m, r * n, s * n))\n",
        "\n",
        "    return full_kernel[:, :, :input_shape[0], :input_shape[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwjwq1q3FtSB"
      },
      "source": [
        "def complex_convolution(inputs, kernel):\n",
        "    '''performs convolution for complex inputs and kernel'''\n",
        "    inputs_real = tf.dtypes.cast(tf.math.real(inputs), tf.float32)\n",
        "    inputs_imag = tf.dtypes.cast(tf.math.imag(inputs), tf.float32)\n",
        "    kernel_real = tf.dtypes.cast(tf.math.real(kernel), tf.float32)\n",
        "    kernel_imag = tf.dtypes.cast(tf.math.imag(kernel), tf.float32)\n",
        "\n",
        "    outputs_real = tf.keras.backend.conv2d(inputs_real, kernel_real, [1] * len(inputs.shape), 'same')\n",
        "    outputs_real -= tf.keras.backend.conv2d(inputs_imag, kernel_imag, [1] * len(inputs.shape), 'same')\n",
        "    outputs_imag = tf.keras.backend.conv2d(inputs_real, kernel_imag, [1] * len(inputs.shape), 'same')\n",
        "    outputs_imag += tf.keras.backend.conv2d(inputs_imag, kernel_real, [1] * len(inputs.shape), 'same')\n",
        "\n",
        "    return tf.complex(outputs_real, outputs_imag)\n",
        "\n",
        "# @tf.custom_gradient # TODO: if we are adding custom gradient calculation, this is to be used\n",
        "def fft_convolution(K, inputs, filters): # TODO: add stride, padding and so on\n",
        "    # getting shapes\n",
        "    k, _, r, s, n = K.shape\n",
        "    batch_size, w, h, c0 = inputs.shape\n",
        "\n",
        "    # padding input to match kernel\n",
        "    inputs_padding = tf.constant([[0, 0], [0, 0], [0, 0], [0, r * n  - c0]])\n",
        "    input = tf.pad(inputs, inputs_padding)\n",
        "    \n",
        "    # fft along the last dimension\n",
        "    input_fft_reshape = tf.signal.fft(tf.dtypes.cast(tf.reshape(input, (-1, w, h, r, n)), tf.complex128))\n",
        "    kernel_fft = tf.signal.fft(tf.dtypes.cast(K, tf.complex128))\n",
        "\n",
        "    # n (* 4) independent convolutions for complex inputs and kernel\n",
        "    outputs = []\n",
        "    # input_fft_reshape = tf.reshape(input_fft, (-1, w, h, r, n))\n",
        "    for i in range(n):\n",
        "        Y = complex_convolution(input_fft_reshape[:, :, :, :, i], kernel_fft[:, :, :, :, i])\n",
        "        outputs.append(Y)\n",
        "\n",
        "    # stacking outputs\n",
        "    outputs = tf.stack(outputs)\n",
        "    outputs = tf.transpose(outputs, [1, 2, 3, 4, 0])\n",
        "\n",
        "    # ifft & final reshape\n",
        "    outputs = tf.math.real(tf.signal.ifft(outputs))\n",
        "    outputs = tf.reshape(outputs, (-1, w, h, s * n))\n",
        "\n",
        "    # truncating outputs to the filter shape\n",
        "    outputs = outputs[:, :, :, :filters]\n",
        "\n",
        "    # TODO: backprop\n",
        "    # def gradient(dL):\n",
        "\n",
        "    return outputs\n",
        "\n",
        "class CircConv2D(tf.keras.layers.Conv2D):\n",
        "    def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               n,   # c0 = R * n, c2 = S * n\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               bias_initializer='zeros',\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "        super(CircConv2D, self).__init__(\n",
        "               filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=strides, # doesn't support strides\n",
        "               padding=padding, # as well as padding\n",
        "               data_format=data_format,\n",
        "               dilation_rate=dilation_rate,\n",
        "               groups=1, # does not support groups!\n",
        "               activation=activation,\n",
        "               use_bias=use_bias,\n",
        "               kernel_initializer=kernel_initializer,\n",
        "               bias_initializer=bias_initializer,\n",
        "               kernel_regularizer=kernel_regularizer,\n",
        "               bias_regularizer=bias_regularizer,\n",
        "               activity_regularizer=activity_regularizer,\n",
        "               kernel_constraint=kernel_constraint,\n",
        "               bias_constraint=bias_constraint,\n",
        "               **kwargs)\n",
        "        self.n = n\n",
        "        self.K = None\n",
        "        self.bias = None\n",
        "        self._convolution_op = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        input_channel = self._get_input_channel(input_shape)\n",
        "\n",
        "        if input_channel < self.n:\n",
        "            print(\"input_channel number is less than n, shrinking n forcibly\")\n",
        "            self.n = input_channel\n",
        "        if self.filters < self.n:\n",
        "            print(\"output_channel number is less than n, shrinking n forcibly\")\n",
        "            self.n = self.filters\n",
        "        r, s = int(np.ceil(input_channel / self.n)), int(np.ceil(self.filters / self.n))\n",
        "\n",
        "        self.K = self.add_weight(\n",
        "            name='K',\n",
        "            shape=(self.kernel_size[0], self.kernel_size[0], r, s, self.n),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "        \n",
        "        # the rest is copied from Conv build function\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "              name='bias',\n",
        "              shape=(self.filters,),\n",
        "              initializer=self.bias_initializer,\n",
        "              regularizer=self.bias_regularizer,\n",
        "              constraint=self.bias_constraint,\n",
        "              trainable=True,\n",
        "              dtype=self.dtype)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        channel_axis = self._get_channel_axis()\n",
        "        self.input_spec = InputSpec(min_ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_channel})\n",
        "\n",
        "        # Convert Keras formats to TF native formats.\n",
        "        if self.padding == 'causal':\n",
        "            tf_padding = 'VALID'  # Causal padding handled in `call`.\n",
        "        elif isinstance(self.padding, six.string_types):\n",
        "            tf_padding = self.padding.upper()\n",
        "        else:\n",
        "            tf_padding = self.padding\n",
        "        tf_dilations = list(self.dilation_rate)\n",
        "        tf_strides = list(self.strides)\n",
        "    \n",
        "        tf_op_name = self.__class__.__name__\n",
        "        if tf_op_name == 'Conv1D':\n",
        "            tf_op_name = 'conv1d'  # Backwards compat.\n",
        "\n",
        "        self._convolution_op = functools.partial(\n",
        "            nn_ops.convolution_v2,\n",
        "            strides=tf_strides,\n",
        "            padding=tf_padding,\n",
        "            dilations=tf_dilations,\n",
        "            data_format=self._tf_data_format,\n",
        "            name=tf_op_name)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # outputs = fft_convolution(self.K, inputs, self.filters)\n",
        "        outputs = self._convolution_op(inputs, full_circ(self.K, (inputs.shape[-1], self.filters)))\n",
        "\n",
        "        if self.use_bias:\n",
        "            output_rank = outputs.shape.rank\n",
        "            if self.rank == 1 and self._channels_first:\n",
        "                # nn.bias_add does not accept a 1D input tensor.\n",
        "                bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
        "                outputs += bias\n",
        "            else:\n",
        "                # Handle multiple batch dimensions.\n",
        "                if output_rank is not None and output_rank > 2 + self.rank:\n",
        "\n",
        "                    def _apply_fn(o):\n",
        "                        return nn.bias_add(o, self.bias, data_format=self._tf_data_format)\n",
        "\n",
        "                    outputs = nn_ops.squeeze_batch_dims(\n",
        "                      outputs, _apply_fn, inner_rank=self.rank + 1)\n",
        "                else:\n",
        "                    outputs = nn.bias_add(\n",
        "                      outputs, self.bias, data_format=self._tf_data_format)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9-GeRsNGII2"
      },
      "source": [
        "### Клиппинг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44umxAxcIIIb"
      },
      "source": [
        "def circulant_sinvals(kernel, input_shape):\n",
        "    kernel_shape = kernel.shape\n",
        "    kernel_tr = tf.transpose(tf.cast(kernel, tf.complex128), [2, 3, 4, 0, 1])\n",
        "\n",
        "    padding = tf.constant([[0, 0], [0, 0], [0, 0],\n",
        "                           [0, input_shape[0] - kernel_shape[0]],\n",
        "                           [0, input_shape[1] - kernel_shape[1]]])\n",
        "\n",
        "    ps = tf.transpose(tf.signal.fft2d(tf.pad(kernel_tr, padding)), [3, 4, 0, 1, 2]) # puv from Sedghi\n",
        "    ps_fft = tf.signal.fft(ps) # diagonilizing puv-s\n",
        "    tensorok = tf.transpose(ps_fft, [0, 1, 4, 2, 3]) # corresponds to moving into blocks in theory\n",
        "    sinval = tf.linalg.svd(tensorok) # final values\n",
        "    return sinval\n",
        "\n",
        "def svd_reconstruction(svds):\n",
        "    u, sigma, v = svds\n",
        "    sigma = tf.cast(sigma, tf.complex128)\n",
        "    m, r, s = u.shape[0], u.shape[-1], v.shape[-2]\n",
        "\n",
        "    # reconstructing svd\n",
        "    rec0 = tf.matmul(u, tf.matmul(tf.linalg.diag(tf.cast(sigma, tf.complex128)), v, adjoint_b=True))\n",
        "\n",
        "    detensorok = tf.transpose(rec0, [0, 1, 3, 4, 2])\n",
        "    ps_ifft = tf.signal.ifft(detensorok)\n",
        "    res = tf.transpose(tf.signal.ifft2d(tf.transpose(ps_ifft, [2, 3, 4, 0, 1])), [3, 4, 0, 1, 2])\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1gzH1kgGMVd"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "class Clipping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, clip_to):\n",
        "        tf.keras.callbacks.Callback.__init__(self)\n",
        "        self.clip_to = clip_to\n",
        "\n",
        "    def on_epoch_end(self, epochs, logs=None):\n",
        "        for layer in self.model.layers:\n",
        "            if layer.name.startswith(\"circ_conv2d\"):\n",
        "                sinvals, us, vs = circulant_sinvals(layer.K, layer.input_shape[1:3])\n",
        "                sinvals_clipped = tf.cast(tf.minimum(sinvals, self.clip_to), tf.complex128)\n",
        "                new_K = svd_reconstruction((us, sinvals_clipped, vs))\n",
        "                k = layer.K.shape[0]\n",
        "                K.set_value(layer.K, new_K[:k, :k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSatFAHUVVZM"
      },
      "source": [
        "### Проверка работоспособности слоя на наборе данных из букв (одна из наших домашек)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfvhZ-l0rsUd"
      },
      "source": [
        "#### Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3tYiEG_VTGT",
        "outputId": "87dcfa40-3d8d-456f-b50f-24c670e5c025"
      },
      "source": [
        "!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz\n",
        "!tar -xvf notMNIST_large.tar.gz >> /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-23 09:13:49--  http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz\n",
            "Resolving yaroslavvb.com (yaroslavvb.com)... 129.121.4.193\n",
            "Connecting to yaroslavvb.com (yaroslavvb.com)|129.121.4.193|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247336696 (236M) [application/x-gzip]\n",
            "Saving to: ‘notMNIST_large.tar.gz’\n",
            "\n",
            "notMNIST_large.tar. 100%[===================>] 235.88M  54.8MB/s    in 4.6s    \n",
            "\n",
            "2021-05-23 09:13:54 (51.0 MB/s) - ‘notMNIST_large.tar.gz’ saved [247336696/247336696]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxgpV6QQVeQU"
      },
      "source": [
        "DATA_DIR = 'notMNIST_large/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo8NwEd7Vrcu"
      },
      "source": [
        "from glob import glob\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "for img_path in glob(f'{DATA_DIR}/**/*.png'):\n",
        "  try:\n",
        "    img = Image.open(img_path)\n",
        "  except:\n",
        "      os.remove(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAIaOAt4Vvji",
        "outputId": "7f0f0fec-388e-4dd2-c026-fa146467d385"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "plt.figure(figsize=(17,10))\n",
        "letter = 'A'\n",
        "img = cv2.imread(os.path.join(DATA_DIR, letter, os.listdir(f'{DATA_DIR}/{letter}/')[1]))\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7b690a3e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1224x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v28-MI9WWPLn"
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.layers.core import Activation, Reshape, Dense, Flatten\n",
        "from keras.layers import Conv2D, MaxPool2D, InputLayer, BatchNormalization, Dropout, GlobalAveragePooling2D, GlobalMaxPool2D\n",
        "from keras.models import Model\n",
        "\n",
        "pic_size = 28\n",
        "n_classes = len(os.listdir(DATA_DIR))\n",
        "\n",
        "def build_model(n):\n",
        "    print(n_classes)\n",
        "    model = keras.Sequential([\n",
        "        CircConv2D(81, 3, n=n,\n",
        "                        input_shape=(pic_size, pic_size, 3),\n",
        "                            data_format=\"channels_last\", activation='relu',\n",
        "                            padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        CircConv2D(27, 3, n=n,\n",
        "                        activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        CircConv2D(9, 3, n=n,\n",
        "                        activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(128, activation='relu'),\n",
        "        keras.layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3EyzG5DWuER",
        "outputId": "b6aef5b2-c346-4f6c-e140-5d98e83d820d"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# https://stackoverflow.com/questions/53037510/can-flow-from-directory-get-train-and-validation-data-from-the-same-directory-in (самый залайканый ответ)\n",
        "\n",
        "\"\"\" Data generators initialization: for train and validation sets \"\"\"\n",
        "generator = ImageDataGenerator(validation_split=0.1, rescale=1./255)\n",
        "train_gen = generator.flow_from_directory(DATA_DIR,\n",
        "                                          target_size=(pic_size, pic_size),\n",
        "                                          class_mode='categorical',\n",
        "                                          subset='training', seed=1)\n",
        "val_gen = generator.flow_from_directory(DATA_DIR,\n",
        "                                        target_size=(pic_size, pic_size),\n",
        "                                        class_mode='categorical',\n",
        "                                        subset='validation', seed=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 476205 images belonging to 10 classes.\n",
            "Found 52909 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmr9s-v4rJu8"
      },
      "source": [
        "#### Модель с обычными свёрточными слоями"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvWuVxCgjTTM",
        "outputId": "1f3a9f95-da24-4e3d-ee9d-c1ff45a681cd"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(81, 3, input_shape=(pic_size, pic_size, 3),\n",
        "                        data_format=\"channels_last\", activation='relu',\n",
        "                        padding='same'),\n",
        "    keras.layers.MaxPooling2D(),\n",
        "    keras.layers.Conv2D(27, 3, activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D(),\n",
        "    keras.layers.Conv2D(9, 3, activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(n_classes, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 81)        2268      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 81)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 27)        19710     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 27)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 9)           2196      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 9)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 81)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               10496     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 35,960\n",
            "Trainable params: 35,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzA9v2bEoWgE",
        "outputId": "df05b024-785f-469b-a994-725e1dfd6f95"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history = model.fit(train_gen, steps_per_epoch=step_size_train, epochs=2,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "14881/14881 [==============================] - 155s 10ms/step - loss: 0.4668 - accuracy: 0.8611 - val_loss: 0.2634 - val_accuracy: 0.9209\n",
            "Epoch 2/2\n",
            "14881/14881 [==============================] - 155s 10ms/step - loss: 0.3038 - accuracy: 0.9073 - val_loss: 0.2489 - val_accuracy: 0.9236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6afRWxdnLIy9"
      },
      "source": [
        "#### n = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHo-BF8iK-7A",
        "outputId": "0df12b49-05b9-4791-b715-cef18bfd26f4"
      },
      "source": [
        "model1 = build_model(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "circ_conv2d (CircConv2D)     (None, 28, 28, 81)        810       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 81)        0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_1 (CircConv2D)   (None, 14, 14, 27)        6588      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 27)          0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_2 (CircConv2D)   (None, 7, 7, 9)           738       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 9)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 81)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               10496     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 19,922\n",
            "Trainable params: 19,922\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3tgja5sLC0H",
        "outputId": "b0e8d683-8cc2-47e4-c651-3e7242a4f696"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history1 = model1.fit(train_gen, steps_per_epoch=step_size_train, epochs=2,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid, callbacks=[Clipping(1)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "14881/14881 [==============================] - 307s 18ms/step - loss: 0.5416 - accuracy: 0.8384 - val_loss: 0.3157 - val_accuracy: 0.9044\n",
            "Epoch 2/2\n",
            "    1/14881 [..............................] - ETA: 7:36 - loss: 2.4347 - accuracy: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14881/14881 [==============================] - 191s 13ms/step - loss: 0.4050 - accuracy: 0.8766 - val_loss: 0.2800 - val_accuracy: 0.9150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLsbq6icxlyN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}