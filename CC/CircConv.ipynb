{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CircConv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "praf7vwRHu5P"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import regularizers\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
        "import six\n",
        "import functools\n",
        "from tensorflow.python.ops import nn, nn_ops\n",
        "\n",
        "np.random.seed(1)   \n",
        "rn.seed(1)   \n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIOsm1xkDbtG"
      },
      "source": [
        "### Циркулянтный свёрточный 2D слой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwjwq1q3FtSB"
      },
      "source": [
        "def complex_convolution(inputs, kernel):\n",
        "    '''performs convolution for complex inputs and kernel'''\n",
        "    inputs_real = tf.dtypes.cast(tf.math.real(inputs), tf.float32)\n",
        "    inputs_imag = tf.dtypes.cast(tf.math.imag(inputs), tf.float32)\n",
        "    kernel_real = tf.dtypes.cast(tf.math.real(kernel), tf.float32)\n",
        "    kernel_imag = tf.dtypes.cast(tf.math.imag(kernel), tf.float32)\n",
        "\n",
        "    outputs_real = tf.keras.backend.conv2d(inputs_real, kernel_real, [1] * len(inputs.shape), 'same')\n",
        "    outputs_real -= tf.keras.backend.conv2d(inputs_imag, kernel_imag, [1] * len(inputs.shape), 'same')\n",
        "    outputs_imag = tf.keras.backend.conv2d(inputs_real, kernel_imag, [1] * len(inputs.shape), 'same')\n",
        "    outputs_imag += tf.keras.backend.conv2d(inputs_imag, kernel_real, [1] * len(inputs.shape), 'same')\n",
        "\n",
        "    return tf.complex(outputs_real, outputs_imag)\n",
        "\n",
        "# @tf.custom_gradient # TODO: if we are adding custom gradient calculation, this is to be used\n",
        "def fft_convolution(K, inputs, filters): # TODO: add stride, padding and so on\n",
        "    # getting shapes\n",
        "    k, _, r, s, n = K.shape\n",
        "    batch_size, w, h, c0 = inputs.shape\n",
        "\n",
        "    # padding input to match kernel\n",
        "    inputs_padding = tf.constant([[0, 0], [0, 0], [0, 0], [0, r * n  - c0]])\n",
        "    input = tf.pad(inputs, inputs_padding)\n",
        "    \n",
        "    # fft along the last dimension\n",
        "    input_fft_reshape = tf.signal.fft(tf.dtypes.cast(tf.reshape(input, (-1, w, h, r, n)), tf.complex128))\n",
        "    kernel_fft = tf.signal.fft(tf.dtypes.cast(K, tf.complex128))\n",
        "\n",
        "    # n (* 4) independent convolutions for complex inputs and kernel\n",
        "    outputs = []\n",
        "    # input_fft_reshape = tf.reshape(input_fft, (-1, w, h, r, n))\n",
        "    for i in range(n):\n",
        "        Y = complex_convolution(input_fft_reshape[:, :, :, :, i], kernel_fft[:, :, :, :, i])\n",
        "        outputs.append(Y)\n",
        "\n",
        "    # stacking outputs\n",
        "    outputs = tf.stack(outputs)\n",
        "    outputs = tf.transpose(outputs, [1, 2, 3, 4, 0])\n",
        "\n",
        "    # ifft & final reshape\n",
        "    outputs = tf.math.real(tf.signal.ifft(outputs))\n",
        "    outputs = tf.reshape(outputs, (-1, w, h, s * n))\n",
        "\n",
        "    # truncating outputs to the filter shape\n",
        "    outputs = outputs[:, :, :, :filters]\n",
        "\n",
        "    # TODO: backprop\n",
        "    # def gradient(dL):\n",
        "\n",
        "    return outputs\n",
        "\n",
        "class CircConv2D(tf.keras.layers.Conv2D):\n",
        "    def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               n,   # c0 = R * n, c2 = S * n\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               bias_initializer='zeros',\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "        super(CircConv2D, self).__init__(\n",
        "               filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=strides, # doesn't support strides\n",
        "               padding=padding, # as well as padding\n",
        "               data_format=data_format,\n",
        "               dilation_rate=dilation_rate,\n",
        "               groups=1, # does not support groups!\n",
        "               activation=activation,\n",
        "               use_bias=use_bias,\n",
        "               kernel_initializer=kernel_initializer,\n",
        "               bias_initializer=bias_initializer,\n",
        "               kernel_regularizer=kernel_regularizer,\n",
        "               bias_regularizer=bias_regularizer,\n",
        "               activity_regularizer=activity_regularizer,\n",
        "               kernel_constraint=kernel_constraint,\n",
        "               bias_constraint=bias_constraint,\n",
        "               **kwargs)\n",
        "        self.n = n\n",
        "        self.K = None\n",
        "        self.bias = None\n",
        "        self._convolution_op = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        input_channel = self._get_input_channel(input_shape)\n",
        "\n",
        "        if input_channel < self.n:\n",
        "            print(\"input_channel number is less than n, shrinking n forcibly\")\n",
        "            self.n = input_channel\n",
        "        if self.filters < self.n:\n",
        "            print(\"output_channel number is less than n, shrinking n forcibly\")\n",
        "            self.n = self.filters\n",
        "        r, s = int(np.ceil(input_channel / self.n)), int(np.ceil(self.filters / self.n))\n",
        "\n",
        "        self.K = self.add_weight(\n",
        "            name='K',\n",
        "            shape=(self.kernel_size[0], self.kernel_size[0], r, s, self.n),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "        \n",
        "        # the rest is copied from Conv build function\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "              name='bias',\n",
        "              shape=(self.filters,),\n",
        "              initializer=self.bias_initializer,\n",
        "              regularizer=self.bias_regularizer,\n",
        "              constraint=self.bias_constraint,\n",
        "              trainable=True,\n",
        "              dtype=self.dtype)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        channel_axis = self._get_channel_axis()\n",
        "        self.input_spec = InputSpec(min_ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_channel})\n",
        "\n",
        "        # Convert Keras formats to TF native formats.\n",
        "        if self.padding == 'causal':\n",
        "            tf_padding = 'VALID'  # Causal padding handled in `call`.\n",
        "        elif isinstance(self.padding, six.string_types):\n",
        "            tf_padding = self.padding.upper()\n",
        "        else:\n",
        "            tf_padding = self.padding\n",
        "        tf_dilations = list(self.dilation_rate)\n",
        "        tf_strides = list(self.strides)\n",
        "    \n",
        "        tf_op_name = self.__class__.__name__\n",
        "        if tf_op_name == 'Conv1D':\n",
        "            tf_op_name = 'conv1d'  # Backwards compat.\n",
        "\n",
        "        self._convolution_op = functools.partial(\n",
        "            nn_ops.convolution_v2,\n",
        "            strides=tf_strides,\n",
        "            padding=tf_padding,\n",
        "            dilations=tf_dilations,\n",
        "            data_format=self._tf_data_format,\n",
        "            name=tf_op_name)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = fft_convolution(self.K, inputs, self.filters)\n",
        "\n",
        "        if self.use_bias:\n",
        "            output_rank = outputs.shape.rank\n",
        "            if self.rank == 1 and self._channels_first:\n",
        "                # nn.bias_add does not accept a 1D input tensor.\n",
        "                bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
        "                outputs += bias\n",
        "            else:\n",
        "                # Handle multiple batch dimensions.\n",
        "                if output_rank is not None and output_rank > 2 + self.rank:\n",
        "\n",
        "                    def _apply_fn(o):\n",
        "                        return nn.bias_add(o, self.bias, data_format=self._tf_data_format)\n",
        "\n",
        "                    outputs = nn_ops.squeeze_batch_dims(\n",
        "                      outputs, _apply_fn, inner_rank=self.rank + 1)\n",
        "                else:\n",
        "                    outputs = nn.bias_add(\n",
        "                      outputs, self.bias, data_format=self._tf_data_format)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSatFAHUVVZM"
      },
      "source": [
        "### Проверка работоспособности слоя на наборе данных из букв (одна из наших домашек)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfvhZ-l0rsUd"
      },
      "source": [
        "#### Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3tYiEG_VTGT",
        "outputId": "1ba17886-a0e8-4c27-96f9-fc8e53f151b6"
      },
      "source": [
        "!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz\n",
        "!tar -xvf notMNIST_large.tar.gz >> /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-12 19:01:22--  http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz\n",
            "Resolving yaroslavvb.com (yaroslavvb.com)... 129.121.4.193\n",
            "Connecting to yaroslavvb.com (yaroslavvb.com)|129.121.4.193|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247336696 (236M) [application/x-gzip]\n",
            "Saving to: ‘notMNIST_large.tar.gz’\n",
            "\n",
            "notMNIST_large.tar. 100%[===================>] 235.88M  22.1MB/s    in 11s     \n",
            "\n",
            "2021-05-12 19:01:34 (21.6 MB/s) - ‘notMNIST_large.tar.gz’ saved [247336696/247336696]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxgpV6QQVeQU"
      },
      "source": [
        "DATA_DIR = 'notMNIST_large/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo8NwEd7Vrcu"
      },
      "source": [
        "from glob import glob\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "for img_path in glob(f'{DATA_DIR}/**/*.png'):\n",
        "  try:\n",
        "    img = Image.open(img_path)\n",
        "  except:\n",
        "      os.remove(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "PAIaOAt4Vvji",
        "outputId": "57942fef-813d-4bcf-87b8-303760856495"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "plt.figure(figsize=(17,10))\n",
        "letter = 'A'\n",
        "img = cv2.imread(os.path.join(DATA_DIR, letter, os.listdir(f'{DATA_DIR}/{letter}/')[1]))\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f71d35fc5d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1224x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v28-MI9WWPLn"
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.layers.core import Activation, Reshape, Dense, Flatten\n",
        "from keras.layers import Conv2D, MaxPool2D, InputLayer, BatchNormalization, Dropout, GlobalAveragePooling2D, GlobalMaxPool2D\n",
        "from keras.models import Model\n",
        "\n",
        "pic_size = 28\n",
        "n_classes = len(os.listdir(DATA_DIR))\n",
        "\n",
        "def build_model(n):\n",
        "    print(n_classes)\n",
        "    model = keras.Sequential([\n",
        "        CircConv2D(81, 3, n=n,\n",
        "                        input_shape=(pic_size, pic_size, 3),\n",
        "                            data_format=\"channels_last\", activation='relu',\n",
        "                            padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        CircConv2D(27, 3, n=n,\n",
        "                        activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        CircConv2D(9, 3, n=n,\n",
        "                        activation='relu', padding='same'),\n",
        "        keras.layers.MaxPooling2D(),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(128, activation='relu'),\n",
        "        keras.layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3EyzG5DWuER",
        "outputId": "64a7f0a7-b529-4b6d-a925-7bee18c504e4"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# https://stackoverflow.com/questions/53037510/can-flow-from-directory-get-train-and-validation-data-from-the-same-directory-in (самый залайканый ответ)\n",
        "\n",
        "\"\"\" Data generators initialization: for train and validation sets \"\"\"\n",
        "generator = ImageDataGenerator(validation_split=0.1, rescale=1./255)\n",
        "train_gen = generator.flow_from_directory(DATA_DIR,\n",
        "                                          target_size=(pic_size, pic_size),\n",
        "                                          class_mode='categorical',\n",
        "                                          subset='training', seed=1)\n",
        "val_gen = generator.flow_from_directory(DATA_DIR,\n",
        "                                        target_size=(pic_size, pic_size),\n",
        "                                        class_mode='categorical',\n",
        "                                        subset='validation', seed=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 476205 images belonging to 10 classes.\n",
            "Found 52909 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmr9s-v4rJu8"
      },
      "source": [
        "#### Модель с обычными свёрточными слоями"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvWuVxCgjTTM",
        "outputId": "30746180-5138-4b67-cc0c-3cdf5869f3ec"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(81, 3, input_shape=(pic_size, pic_size, 3),\n",
        "                        data_format=\"channels_last\", activation='relu',\n",
        "                        padding='same'),\n",
        "    keras.layers.MaxPooling2D(),\n",
        "    keras.layers.Conv2D(27, 3, activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D(),\n",
        "    keras.layers.Conv2D(9, 3, activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(n_classes, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 81)        2268      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 14, 14, 81)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 27)        19710     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 7, 7, 27)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 9)           2196      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 3, 3, 9)           0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 81)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               10496     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 35,960\n",
            "Trainable params: 35,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzA9v2bEoWgE",
        "outputId": "df05b024-785f-469b-a994-725e1dfd6f95"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history = model.fit(train_gen, steps_per_epoch=step_size_train, epochs=2,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "14881/14881 [==============================] - 155s 10ms/step - loss: 0.4668 - accuracy: 0.8611 - val_loss: 0.2634 - val_accuracy: 0.9209\n",
            "Epoch 2/2\n",
            "14881/14881 [==============================] - 155s 10ms/step - loss: 0.3038 - accuracy: 0.9073 - val_loss: 0.2489 - val_accuracy: 0.9236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6afRWxdnLIy9"
      },
      "source": [
        "#### n = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHo-BF8iK-7A",
        "outputId": "2a2240a7-3a44-461c-d093-e7031948223c"
      },
      "source": [
        "model1 = build_model(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "circ_conv2d_9 (CircConv2D)   (None, 28, 28, 81)        2268      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 81)        0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_10 (CircConv2D)  (None, 14, 14, 27)        19710     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 27)          0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_11 (CircConv2D)  (None, 7, 7, 9)           2196      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 3, 3, 9)           0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 81)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               10496     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 35,960\n",
            "Trainable params: 35,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3tgja5sLC0H",
        "outputId": "dc1d3aab-a4c7-4597-c76b-146a2a3406ce"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history1 = model1.fit(train_gen, steps_per_epoch=step_size_train, epochs=2,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "14881/14881 [==============================] - 253s 17ms/step - loss: 0.4828 - accuracy: 0.8564 - val_loss: 0.2640 - val_accuracy: 0.9205\n",
            "Epoch 2/2\n",
            "14881/14881 [==============================] - 252s 17ms/step - loss: 0.3025 - accuracy: 0.9079 - val_loss: 0.2449 - val_accuracy: 0.9266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRChluDlrVYz"
      },
      "source": [
        "#### n = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUuo_lwPigPL",
        "outputId": "c77516f7-3c45-48b9-d5f9-72ec03f741a8"
      },
      "source": [
        "model3 = build_model(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "circ_conv2d (CircConv2D)     (None, 28, 28, 81)        810       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 81)        0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_1 (CircConv2D)   (None, 14, 14, 27)        6588      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 27)          0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_2 (CircConv2D)   (None, 7, 7, 9)           738       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 9)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 81)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               10496     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 19,922\n",
            "Trainable params: 19,922\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR4MyUYOburL",
        "outputId": "b5e9693f-c3c1-4dfa-c212-1686bc7bf243"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history2 = model3.fit(train_gen, steps_per_epoch=step_size_train, epochs=2,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "14881/14881 [==============================] - 400s 25ms/step - loss: 0.5386 - accuracy: 0.8391 - val_loss: 0.3197 - val_accuracy: 0.9029\n",
            "Epoch 2/2\n",
            "14881/14881 [==============================] - 350s 24ms/step - loss: 0.3571 - accuracy: 0.8912 - val_loss: 0.2807 - val_accuracy: 0.9154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3VcnSJ9rahO"
      },
      "source": [
        "#### n = 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxOR56Sji3XJ",
        "outputId": "30b33912-a037-47df-9445-6aedb487753e"
      },
      "source": [
        "model10 = build_model(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "input_channel number is less than n, shrinking n forcibly\n",
            "output_channel number is less than n, shrinking n forcibly\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "circ_conv2d_3 (CircConv2D)   (None, 28, 28, 81)        810       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 81)        0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_4 (CircConv2D)   (None, 14, 14, 27)        2457      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 27)          0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_5 (CircConv2D)   (None, 7, 7, 9)           252       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 9)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 81)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               10496     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 15,305\n",
            "Trainable params: 15,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a5QMldUb09e",
        "outputId": "ff672886-1849-452f-c6be-f0d8868ac6c9"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history2 = model10.fit(train_gen, steps_per_epoch=step_size_train, epochs=2,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "14881/14881 [==============================] - 561s 37ms/step - loss: 0.5370 - accuracy: 0.8376 - val_loss: 0.3001 - val_accuracy: 0.9100\n",
            "Epoch 2/2\n",
            "14881/14881 [==============================] - 554s 37ms/step - loss: 0.3352 - accuracy: 0.8976 - val_loss: 0.2749 - val_accuracy: 0.9166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Yu9eW78qoW"
      },
      "source": [
        "#### n = c0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnDatPu59RUc",
        "outputId": "4238aa22-6bca-4945-c8e3-077d8ae3dbd9"
      },
      "source": [
        "model1000 = build_model(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "input_channel number is less than n, shrinking n forcibly\n",
            "input_channel number is less than n, shrinking n forcibly\n",
            "output_channel number is less than n, shrinking n forcibly\n",
            "input_channel number is less than n, shrinking n forcibly\n",
            "output_channel number is less than n, shrinking n forcibly\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "circ_conv2d_6 (CircConv2D)   (None, 28, 28, 81)        810       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 81)        0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_7 (CircConv2D)   (None, 14, 14, 27)        756       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 27)          0         \n",
            "_________________________________________________________________\n",
            "circ_conv2d_8 (CircConv2D)   (None, 7, 7, 9)           252       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 3, 3, 9)           0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 81)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               10496     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 13,604\n",
            "Trainable params: 13,604\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAEir6_99jvA",
        "outputId": "70162ffa-cee5-45c4-a95f-341781489302"
      },
      "source": [
        "step_size_train = (train_gen.n // train_gen.batch_size)\n",
        "step_size_valid = (val_gen.n // val_gen.batch_size)\n",
        "\n",
        "history3 = model1000.fit(train_gen, steps_per_epoch=step_size_train, epochs=2,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=step_size_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "14881/14881 [==============================] - 789s 53ms/step - loss: 0.5298 - accuracy: 0.8422 - val_loss: 0.3052 - val_accuracy: 0.9068\n",
            "Epoch 2/2\n",
            "14881/14881 [==============================] - 785s 53ms/step - loss: 0.3455 - accuracy: 0.8946 - val_loss: 0.2921 - val_accuracy: 0.9124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNSfCmM2jrJF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}